# test-discovery.conv - Analyze sdqctl source for test requirements
# 
# Purpose: Dogfooding workflow that uses sdqctl to analyze itself,
# discovering requirements, implementation gaps, and test recommendations.
#
# Usage:
#   cd /path/to/sdqctl
#   sdqctl run examples/workflows/test-discovery.conv --adapter mock --dry-run
#   sdqctl run examples/workflows/test-discovery.conv --adapter copilot
#
# Design principles (following QUIRKS.md Q-001/Q-003 guidance):
# 1. Explicit role clarification in PROLOGUE
# 2. Uses "audit" MODE for analysis work
# 3. Literal descriptions in HEADER (not {{WORKFLOW_NAME}})
# 4. Safe template variables only ({{DATE}}, {{GIT_BRANCH}}, etc.)

MODEL gpt-4
ADAPTER copilot
MODE audit
MAX-CYCLES 1

# === Role Clarification ===
# Explicit analysis role to ensure focused output
PROLOGUE You are a test requirements analyst. Your job is to:
PROLOGUE 1. Identify gaps between documentation and implementation
PROLOGUE 2. Recommend specific test cases with priorities
PROLOGUE 3. Cite file:line references for all findings
PROLOGUE 
PROLOGUE Project: sdqctl - vendor-agnostic AI workflow orchestration tool
PROLOGUE Analysis date: {{DATE}}
PROLOGUE Git branch: {{GIT_BRANCH}} ({{GIT_COMMIT}})

# Remind AI to be specific after every prompt
EPILOGUE Cite specific line numbers and file paths in your analysis.
EPILOGUE Format findings as actionable items with priority (P0=critical, P1=high, P2=medium).

# === Source Code Context ===
# NOTE: These CONTEXT directives inject ~4000 lines upfront.
# See docs/CONTEXT-MANAGEMENT.md for the tradeoffs.
# Alternative: Comment out and let agent read on demand.
# Core implementation files
CONTEXT @sdqctl/core/conversation.py
CONTEXT @sdqctl/commands/run.py

# === Documentation Context ===
# Proposals and integration docs
CONTEXT @INTEGRATION-PROPOSAL.md
CONTEXT @README.md
CONTEXT @TEST-PLAN.md

# === Example Workflows ===
CONTEXT @examples/workflows/verify-with-run.conv

# === Pre-analysis Validation ===
# Run validation to capture current state
RUN-ON-ERROR continue
RUN-OUTPUT always

# Validate the example workflow parses correctly
RUN python3 -c "
from sdqctl.core.conversation import ConversationFile
from pathlib import Path
conv = ConversationFile.from_file(Path('examples/workflows/verify-with-run.conv'))
print(f'Parsed: {len(conv.prompts)} prompts, {len(conv.steps)} steps')
print(f'Prologues: {conv.prologues}')
print(f'Epilogues: {conv.epilogues}')
print(f'Headers: {conv.headers}')
print(f'Footers: {conv.footers}')
print(f'RUN on error: {conv.run_on_error}')
print(f'RUN output: {conv.run_output}')
"

# === Analysis Prompts ===

PROMPT ## Requirements Extraction

Analyze the source files provided and extract:

1. **Documented Requirements** - What capabilities are described in INTEGRATION-PROPOSAL.md and README.md?
2. **Implemented Features** - What is actually implemented in conversation.py and run.py?
3. **Requirements Gap** - What is documented but not implemented?
4. **Implementation Gap** - What is implemented but not documented?

Focus on the new ergonomic features:
- PROLOGUE/EPILOGUE directives
- HEADER/FOOTER directives  
- RUN/RUN-ON-ERROR/RUN-OUTPUT directives
- Template variable substitution

PROMPT ## Test Recommendations

Based on the implementation in conversation.py, recommend pytest test cases:

1. **Unit Tests for Directive Parsing**
   - Test each DirectiveType is parsed correctly
   - Test edge cases (empty values, @file references, multiple occurrences)
   
2. **Unit Tests for Template Variables**
   - Test get_standard_variables() returns expected keys
   - Test substitute_template_variables() handles missing keys gracefully
   - Test resolve_content_reference() for both inline and @file cases

3. **Integration Tests for Workflow Execution**
   - Test PROLOGUE content is prepended to prompts
   - Test EPILOGUE content is appended to prompts
   - Test HEADER/FOOTER appear in output
   - Test RUN commands execute and capture output

For each test, provide:
- Test name (test_xxx_yyy)
- Brief description
- Key assertions
- Priority (P0/P1/P2)

PROMPT ## Implementation Quality Review

Review the code quality of the new features:

1. **Error Handling**
   - Are exceptions properly caught?
   - Are error messages user-friendly?
   - Is there proper fallback behavior?

2. **Edge Cases**
   - What happens with empty PROLOGUE/EPILOGUE?
   - What happens if RUN command times out?
   - What if @file reference doesn't exist?

3. **Security Considerations**
   - Is RUN directive safe for CI/CD use?
   - Are there path traversal risks in @file references?

4. **Code Consistency**
   - Do new features follow existing patterns?
   - Is naming consistent with existing directives?

# === Output Configuration ===
HEADER # sdqctl Test Discovery Report
HEADER ## Session: {{DATETIME}}
HEADER **Branch:** {{GIT_BRANCH}}
HEADER ---

FOOTER ---
FOOTER ## Next Steps
FOOTER 1. Create test files based on recommendations
FOOTER 2. Run tests with pytest
FOOTER 3. Update documentation for any gaps found
FOOTER 
FOOTER *Generated by sdqctl test-discovery workflow*

OUTPUT-FORMAT markdown
OUTPUT-FILE reports/test-discovery-{{DATE}}.md
