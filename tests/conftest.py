"""
Pytest configuration and shared fixtures for sdqctl tests.
"""

import pytest
from pathlib import Path
import tempfile


# =============================================================================
# Session-Scoped Fixtures (shared across entire test session)
# =============================================================================


@pytest.fixture(scope="session")
def session_workspace(tmp_path_factory):
    """Session-scoped workspace for tests that don't modify files.
    
    Persists across all tests in the session, reducing filesystem overhead.
    Use for read-only tests or tests that create isolated subdirectories.
    """
    workspace = tmp_path_factory.mktemp("session_workspace")
    
    # Create common test file structure
    (workspace / "lib").mkdir()
    (workspace / "lib" / "auth.js").write_text("// auth code\nfunction login() {}")
    (workspace / "lib" / "utils.js").write_text("// utils code\nfunction format() {}")
    (workspace / "lib" / "secret.js").write_text("// secret - should be denied")
    (workspace / "tests").mkdir()
    (workspace / "tests" / "auth.test.js").write_text("// auth tests")
    (workspace / "src").mkdir()
    (workspace / "src" / "main.py").write_text("# main app")
    
    return workspace


@pytest.fixture(scope="session")
def shared_mock_adapter():
    """Session-scoped mock adapter for tests that don't need isolation.
    
    Avoids repeated adapter instantiation overhead.
    """
    from sdqctl.adapters.mock import MockAdapter
    return MockAdapter()


@pytest.fixture(scope="session")
def shared_adapter_config():
    """Session-scoped adapter config for common test scenarios."""
    from sdqctl.adapters.base import AdapterConfig
    return AdapterConfig(model="test-model")


# =============================================================================
# Function-Scoped Fixtures (per-test isolation)
# =============================================================================


@pytest.fixture
def temp_workspace(tmp_path):
    """Create a temporary workspace with test files."""
    # Create test file structure
    (tmp_path / "lib").mkdir()
    (tmp_path / "lib" / "auth.js").write_text("// auth code\nfunction login() {}")
    (tmp_path / "lib" / "utils.js").write_text("// utils code\nfunction format() {}")
    (tmp_path / "lib" / "secret.js").write_text("// secret - should be denied")
    (tmp_path / "tests").mkdir()
    (tmp_path / "tests" / "auth.test.js").write_text("// auth tests")
    (tmp_path / "src").mkdir()
    (tmp_path / "src" / "main.py").write_text("# main app")
    return tmp_path


@pytest.fixture
def sample_conv_content():
    """Minimal valid .conv file content."""
    return """MODEL gpt-4
ADAPTER mock
MODE audit
PROMPT Analyze the code.
"""


@pytest.fixture
def complex_conv_content():
    """Full-featured .conv file content."""
    return """MODEL gpt-4
ADAPTER mock
MODE audit
MAX-CYCLES 2

CONTEXT @lib/*.js
CONTEXT-LIMIT 80%
ON-CONTEXT-LIMIT compact

ALLOW-FILES lib/*.js
DENY-FILES lib/secret.js

PROLOGUE Analysis date: 2026-01-20
EPILOGUE Remember to cite line numbers.

PROMPT Analyze code quality.
CHECKPOINT quality-analysis
PROMPT Generate improvement recommendations.
COMPACT findings, recommendations

HEADER # Quality Report
FOOTER ---
FOOTER Generated by sdqctl

OUTPUT-FORMAT markdown
OUTPUT-FILE reports/quality-2026-01-20.md
"""


@pytest.fixture
def multiline_conv_content():
    """Conversation file with multiline prompts."""
    return """MODEL gpt-4
ADAPTER mock

PROMPT Analyze the code structure.
  Focus on:
  - Authentication flow
  - Error handling
  - Performance concerns
PROMPT Write recommendations.
"""


@pytest.fixture
def file_restrictions_conv_content():
    """Conversation file testing file restrictions."""
    return """MODEL gpt-4
ADAPTER mock

ALLOW-FILES lib/*.js
ALLOW-FILES src/*.py
DENY-FILES lib/secret.js
DENY-FILES **/test_*.py
ALLOW-DIR lib
DENY-DIR node_modules

PROMPT Analyze allowed files.
"""


@pytest.fixture
def steps_conv_content():
    """Conversation file with various step types."""
    return """MODEL gpt-4
ADAPTER mock

PROMPT First analysis prompt.
CHECKPOINT after-analysis
PROMPT Second prompt with improvements.
COMPACT findings
RUN npm run test
PROMPT Verify test results.
"""


@pytest.fixture
def pause_conv_content():
    """Conversation file with PAUSE directive."""
    return """MODEL gpt-4
ADAPTER mock

PROMPT Analyze the authentication code.
PAUSE Review findings before proceeding
PROMPT Generate final report.
"""


@pytest.fixture
def consult_conv_content():
    """Conversation file with CONSULT directive."""
    return """MODEL gpt-4
ADAPTER mock
SESSION-NAME feature-design

PROMPT Analyze this proposal and identify open questions.
CONSULT Design Decisions
PROMPT Implement the resolved design.
"""


# CLI test fixtures
from click.testing import CliRunner


@pytest.fixture
def cli_runner():
    """Click test runner for CLI testing."""
    return CliRunner()


@pytest.fixture
def workflow_file(tmp_path):
    """Create a valid workflow file for testing."""
    content = """MODEL gpt-4
ADAPTER mock
PROMPT Test prompt.
"""
    f = tmp_path / "test.conv"
    f.write_text(content)
    return f


@pytest.fixture
def run_workflow_file(tmp_path):
    """Create a workflow with RUN directive for testing."""
    content = """MODEL gpt-4
ADAPTER mock
RUN echo "hello world"
PROMPT Check the output.
"""
    f = tmp_path / "run-test.conv"
    f.write_text(content)
    return f


# Re-export fixtures from fixtures package
pytest_plugins = ["tests.fixtures"]
